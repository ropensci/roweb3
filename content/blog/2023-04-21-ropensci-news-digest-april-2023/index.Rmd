---
title: rOpenSci News Digest, April 2023
author:
  - The rOpenSci Team
date: '2023-04-21'
slug: ropensci-news-digest-april-2023
categories: []
tags:
  - newsletter
description: Interview of Athanasia Monika Mowinckel, multilingual blog, coworking, R-universe, new packages and package news
params:
  last_newsletter: '2023-03-17'
---

```{r setup, include=FALSE}
library("magrittr")
library("rlang")
last_newsletter <- anytime::anytime(params$last_newsletter)
knitr::opts_chunk$set(echo = FALSE)
url <- sprintf(
    "/blog/%s/%s/%s/%s",
    lubridate::year(rmarkdown::yaml_front_matter(knitr::current_input())$date),
    stringr::str_pad(lubridate::month(rmarkdown::yaml_front_matter(knitr::current_input())$date), 2, "0", side = "left"),
    stringr::str_pad(lubridate::day(rmarkdown::yaml_front_matter(knitr::current_input())$date), 2, "0", side = "left"),
    rmarkdown::yaml_front_matter(knitr::current_input())$slug
    )
english <- function(x) {
  as.character(english::english(x))
}

nice_string <- function(...) {
  glue::glue_collapse(..., sep = ", ", last = ", and ")
}
```
<!-- Before sending DELETE THE INDEX_CACHE and re-knit! -->

Dear rOpenSci friends, it's time for our monthly news roundup!
<!-- blabla -->
You can read this post [on our blog](`r url`).
Now let's dive into the activity at and around rOpenSci!

## rOpenSci HQ

### Meeting the stars of the R-universe: Athanasia Monika Mowinckel

Knowing our community's stories helps us to learn about the people behind our software, brings us closer and offers us new opportunities. To share some of these community stories, we created the rOpenSci interview series [_"Meeting the stars of the R-Universe"_](/tags/r-universe-stars/).

Working with the human brain requires specific software and complex developments. However R appears to be the natural way to collect and analyze the huge amount of data needed, and the R-Universe the most friendly place to share and invite collaborations. Our third stop brings us to Norway to talk with [Athanasia Monika Mowinckel](/blog/2023/03/30/r-universe-stars-3-en/).
You can also read the post in [Spanish](/es/blog/2023/03/30/r-universe-stars-3-es/).


### Multilingual posts

Have you noticed several of our recent blog posts were translated to Spanish, and in one case, French?
We've started adding actual multilingual infrastructure to our website.
You can find all posts in Spanish in <https://ropensci.org/es/archive> (and the lone French post in <https://ropensci.org/fr/archive>).
From any post that has a translation, the sidebar on the right (or... at the bottom if you're reading on mobile!) has a link to translations of the post, see this [example](/blog/2023/02/28/r-universe-stars-2-en/).

Over time we plan to further improve the multilingualism of our website.

### 18,000 packages tracked on R-Universe!

Currently R-Universe is tracking and indexing over 18,000 R packages! These are a mix of packages found on popular networks like [CRAN](https://cran.r-project.org/) or [Bioconductor](https://www.bioconductor.org/), and packages that were registered by users.

In previous posts we explained [how to create your personal CRAN-like repository](/blog/2021/06/22/setup-runiverse/) and publish packages on R-Universe yourself. A new post explains the other part: how the scraper [automatically finds packages on CRAN and Bioconductor](/blog/2023/04/03/cran-to-git/) to include in the R-Universe.

All in all, R-Universe provides a great way to [discover and explore many packages](/blog/2023/02/27/runiverse-discovering/)!

### Help us put together the rOpenSci Community Call calendar for the year!

We've had [some](https://ropensci.org/commcalls/oct2022-champions/) [great](https://ropensci.org/commcalls/jan2023-targets/) community calls lately,  ðŸŽ‰  
and now we'd like to know what topics **you** would like us to cover in the future!

We hold [Community Calls](/commcalls/) about every 3 months to share knowledge that is relevant to our community and consistent with [our vision and mission](/about/). These are free and open for anyone to attend and provide opportunities for us to connect with rOpenSci community members around the world.

Weâ€™ve [opened an issue in this repository for each topic weâ€™re considering](https://github.com/ropensci-org/community-calls/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc). We'd like **your input and "votes"** on these, and **your suggestions for other topics** we haven't thought of.

We will also be very grateful if you suggest speakers, resources, or encourage others to weigh in by sharing the link to your favorite topic.

You can [read more details on our community calls repository](https://github.com/ropensci-org/community-calls). 

We look forward to hearing your ideas. Let's build the schedule together!    

### Coworking

Join us for social coworking & office hours monthly on first Tuesdays! 
Hosted by Steffi LaZerte and various community hosts. 
Everyone welcome. 
No RSVP needed. 
Consult our [Events](/events) page to find your local time and how to join.
    
And remember, you can always cowork independently on work related to R, work on packages that tend to be neglected, or work on what ever you need to get done!

* Tuesday, May 2nd, 9:00 Americas Pacific / 16:00 UTC ["Spring Cleaning for R packages and scripts"](/events/coworking-2023-05/) *Hosted by [Steffi LaZerte](/author/steffi-lazerte/) and [Yanina Bellini Saibene](/author/yanina-bellini-saibene/)*
    - Explore how other organizations keep their scripts/packages nice and clean
    - Take a look at your R packages and scripts and give them a good spring cleaning\*
    - Talk to our community host and other attendees and discuss tips for keeping on top of it all.
    
\* in the northern hemisphere at least, otherwise, give them a good *fall* cleaning!

* Tuesday, June 6th, 9:00 Australia Western / 01:00 UTC ["Integrating and merging datasets from different sources"](/events/coworking-2023-06/) *Hosted by community host [Cynthia Huang](/author/cynthia-huang) and [Steffi LaZerte](/author/steffi-lazerte/)*
    - Spend time integrating datasets for your own work
    - Explore how others tackle the problem of merging datasets from different sources or different data versions
    - Talk to our community host and other attendees and discuss strategies for integrating datasets.
    
## Software :package:

### New packages

```{r new-packages, cache = TRUE}
cran_unquote <- function(string) {
  gsub("\\'(.*?)\\'", "\\1", string)
}
tidy_package <- function(entry) {
  tibble::tibble(
    package = entry$name,
    description = cran_unquote(entry$description),
    details = cran_unquote(entry$details),
    on_cran = entry$on_cran,
    on_bioc = entry$on_bioc,
    onboarding = entry$onboarding,
    url = entry$url,
    maintainer = entry$maintainer # use desc for more info
    
  )
}

registry <- "https://raw.githubusercontent.com/ropensci/roregistry/gh-pages/registry.json" %>%
  jsonlite::read_json() %>%
  purrr::pluck("packages") %>%
  purrr::map_df(tidy_package)
  
since <- lubridate::as_date(last_newsletter) - 1
until <- lubridate::as_date(last_newsletter) + 1
commits <- gh::gh(
  "GET /repos/{owner}/{repo}/commits",
  owner = "ropensci",
  repo = "roregistry",
  since = sprintf(
    "%s-%s-%sT00:00:00Z",
    lubridate::year(since),
    stringr::str_pad(lubridate::month(since), 2, "0", side = "left"),
    stringr::str_pad(lubridate::day(since), 2, "0", side = "left")
  ),
  until = sprintf(
    "%s-%s-%sT00:00:00Z",
    lubridate::year(until),
    stringr::str_pad(lubridate::month(until), 2, "0", side = "left"),
    stringr::str_pad(lubridate::day(until), 2, "0", side = "left")
  )
)

empty <- TRUE
i <- length(commits)
while (empty == TRUE) {
  old <- "https://raw.githubusercontent.com/ropensci/roregistry/%s/packages.json" %>%
    sprintf(commits[[i]]$sha) %>%
    jsonlite::read_json() %>%
    purrr::map_df(function(x) tibble::tibble(package = x$package, url = x$url, branch = x$branch))
  i <- i - 1
  if (nrow(old) > 100) {
    empty <- FALSE
  }
}

old <- dplyr::filter(
  old,
  !grepl("ropenscilabs\\/", url),
  !grepl("ropensci-archive\\/", url)
)

new <- dplyr::filter(
  registry,
  !package %in% c("sofa", "geojson", old$package),
  !grepl("ropenscilabs\\/", url),
  !grepl("ropensci-archive\\/", url)
)
```


The following `r if(nrow(new)>1) english(nrow(new))` package`r if(nrow(new)>1) "s"` recently became a part of our software suite:

```{r, results='asis', cache = TRUE}
packages <- split(new, seq(nrow(new)))
present_one <- function(package) {
  url_parts <- urltools::url_parse(package$url)
  desc_link <- gh::gh(
    "/repos/{owner}/{repo}/contents/{path}",
    owner = strsplit(url_parts$path, "\\/")[[1]][1],
    repo = strsplit(url_parts$path, "\\/")[[1]][2],
    path = "DESCRIPTION"
  ) %>%
    purrr::pluck("download_url")
  withr::with_tempfile(
    "tf", {
      download.file(desc_link, tf) 
      desc <<- desc::desc(file = tf)
    }
  )
  # as in pkgdown
  authors <- unclass(desc$get_authors())
  aut <- purrr::keep(authors, function(x) {any( x$role %in% "aut") && all(x$role != "cre") })
  aut <- purrr::map_chr(aut, function(x) paste(x$given, x$family))
  rev <- purrr::keep(authors, function(x) {any( x$role %in% "rev") && all(x$role != "cre") })
  rev <- purrr::map_chr(rev, function(x) paste(x$given, x$family))
  maintainer <- purrr::keep(authors, function(x) {any( x$role %in% "cre") })
  maintainer <- paste(c(maintainer[[1]]$given, maintainer[[1]]$family), collapse = " ")
  
  author_string <- sprintf("developed by %s", maintainer)
  
  if (length(aut) > 0) {
    author_string <- paste0(author_string, sprintf(" together with %s", nice_string(aut)))
  } 
  
  string <- sprintf(
    "[%s](https://docs.ropensci.org/%s), %s: %s. ",
    package$package, 
    package$package, 
    author_string,
    stringr::str_remove(stringr::str_squish(package$details), "\\.$")
  )
  
  if (package$on_cran) {
    string <- paste0(
      string, 
      sprintf(
        " It is available on [CRAN]( https://CRAN.R-project.org/package=%s). ",
        package$package
      )
    )
  }
  if (package$on_bioc) {
    string <- paste0(
      string, sprintf(
        " It is available on [Bioconductor](https://bioconductor.org/packages/%s/). ",
        package$package
      )
    )
  }
  if (nzchar(package$onboarding)) {
    string <- paste0(string, sprintf("It has been [reviewed](%s)", package$onboarding))
    if (length(rev) > 0) {
      string <- paste0(string, sprintf(" by %s.", nice_string(rev)))
    } else {
      string <- paste0(string, ".")
    }
  }
  
  paste("+", string)

}
text <- purrr::map_chr(
  packages,
  present_one
)
cat(paste0(text, collapse = "\n\n"))
```

Discover [more packages](/packages), read more about [Software Peer Review](/software-review).

### New versions

```{r news, cache=TRUE}
registry <- dplyr::filter(
  registry,
  !grepl("ropenscilabs\\/", url),
  !grepl("ropensci-archive\\/", url)
)

registry <- registry %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
  owner = strsplit(urltools::path(url), "/")[[1]][1],
  repo = strsplit(urltools::path(url), "/")[[1]][2]
) %>%
  dplyr::filter(
    !is.na(owner)
  )
registry[registry[["package"]] == "commonmark",][["owner"]] <- "r-lib"
registry[registry[["package"]] == "commonmark",][["repo"]] <- "commonmark"
packages <- split(registry, seq(nrow(registry)))
get_release <- function(repo) {
  info <- gh::gh(
    "GET /repos/{owner}/{repo}/releases",
    owner = repo$owner,
    repo = repo$repo,
    per_page = 3,
    description = repo$description
  )
  info <- info[!purrr::map_lgl(info, "draft")]
  if(length(info) == 0 || anytime::anytime(info[[1]]$published_at) < last_newsletter) {
    return(NULL)
  }
  
  tibble::tibble(
    package = repo$package,
    version = info[[1]]$tag_name,
    url = info[[1]]$html_url,
    description = repo$description
  )
}
releases <- purrr::map_df(
  packages,
  get_release
)
releases <- split(releases, seq(nrow(releases)))
format_release <- function(release) {
  sprintf(
    '[%s](https://docs.ropensci.org/%s "%s") ([`%s`](%s))',
    release$package,
    release$package,
    release$description,
    release$version,
    release$url
  )
}
all_releases <- purrr::map_chr(releases, format_release)
text <- nice_string(all_releases)
```

The following `r if (length(releases) > 1) english(length(releases))` package`r if (length(releases) > 1) "s"` `r if (length(releases) > 1) "have" else "has"` had an update since the last newsletter: `r text`.
## Software Peer Review

```{r software-review, results='asis'}
# from pkgdown https://github.com/r-lib/pkgdown/blob/1ca166905f1b019ed4af9642617ea09fa2b8fc17/R/utils.r#L176

get_description <- function(body) {
  lines <- strsplit(body, "\n")[[1]]
  name <- stringr::str_squish(sub("Package:", "", lines[grepl("^Package", lines)][1]))
  description <- stringr::str_squish(sub("Title:", "", lines[grepl("^Title", lines)][1]))
  description <- cran_unquote(sub("\\.$", "", description))
  list(name = name, description = description)
}

get_user_text <- function(issue) {
  info <- gh::gh("GET /users/{username}", username = issue$user$login)
  name <- info$name %||% issue$user$login
  url <- if (nzchar(info$blog)) info$blog else info$html_url
  if (!grepl("^https?:", url)) url <- paste0("http://", url)
  sprintf("[%s](%s)", name, url)
  
}

tidy_issue <- function(issue) {
  labels <- purrr::map_chr(issue$labels, "name")
  label <- labels[grepl("[0-9]\\/.*", labels)][1]
  df <- tibble::tibble(
    label = label,
    name = get_description(issue$body)$name,
    description = get_description(issue$body)$description,
    title = issue$title,
    holding = "holding" %in% purrr::map_chr(issue$labels, "name"),
    others = toString(purrr::map_chr(issue$labels, "name")),
    closed_at = issue$closed_at %||% NA,
    url = issue$html_url,
    user = get_user_text(issue),
    stats = dplyr::if_else("stats" %in% purrr::map_chr(issue$labels, "name"), " (Stats).", "")
  )
  
  dplyr::rowwise(df) %>%
    dplyr::mutate(text = sprintf("    * [%s](%s), %s. Submitted by %s. %s", name, url, description, user, stats))
}

get_issues <- function(label, state) {
  issues <- gh::gh(
    "GET /repos/{owner}/{repo}/issues",
    owner = "ropensci",
    repo = "software-review",
    state = state, 
    labels = label
  )
  
  purrr::map_df(issues, tidy_issue)
}
  
active_issues <- purrr::map_df(
  c("1/editor-checks","2/seeking-reviewer(s)","3/reviewer(s)-assigned","4/review(s)-in-awaiting-changes","5/awaiting-reviewer(s)-response","6/approved"),
  get_issues,
  state = "open"
)

closed_issues <- get_issues(state = "closed", label  ="6/approved")

ok_date <- function(date) {
  if (is.na(date)) {
    return(TRUE)
  } 
  
  anytime::anytime(date) >= last_newsletter
}

closed_issues <- dplyr::rowwise(closed_issues) %>%
  dplyr::filter(ok_date(closed_at))

issues <- dplyr::bind_rows(active_issues, closed_issues)


no_holding <- sum(issues$holding)
issues <- dplyr::filter(issues, !holding)
text <- sprintf("There are %s recently closed and active submissions", english(nrow(issues)))
if (no_holding > 0) {
  text <- paste0(
    text,
    sprintf(
      " and %s submission%s on hold.",
      no_holding,
      if (no_holding > 1) "s" else ""
    )
  )
} else {
  text <- paste0(text, ".")
}

count_label <- function(label) {
  no <- snakecase::to_sentence_case(english(sum(issues$label == label, na.rm = TRUE)))
  url <- paste0("https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A", label)
  sprintf("* %s at ['%s'](%s):\n\n %s", no, label, url, paste0(issues$text[!is.na(issues$label)][ issues$label == label], collapse = "\n\n"))
}

cat(text)
cat(
  paste0(
    " Issues are at different stages: \n\n",
    paste0(
      purrr::map_chr(sort(unique(issues$label[!is.na(issues$label)]), decreasing = TRUE), count_label),
      collapse = "\n\n"
    )
  )
)
```

Find out more about [Software Peer Review](/software-review) and how to get involved.

## On the blog

<!-- Do not forget to rebase your branch! -->

```{r blog}

parse_one_post <- function(path){
  lines <- suppressWarnings(readLines(path, encoding = "UTF-8"))
  yaml <- blogdown:::split_yaml_body(lines)$yaml
  yaml <- glue::glue_collapse(yaml, sep = "\n")
  yaml <- yaml::yaml.load(yaml)
  
  language <- function(path) {
    name <- fs::path_ext_remove(fs::path_file(path))
    if (grepl("\\.[a-z][a-z]", name)) {
      sub(".*\\.", "", name)
    } else {
      "en"
    }
  }

  
  meta <- tibble::tibble(
    date = anytime::anydate(yaml$date),
    author = nice_string(yaml$author),
    title = yaml$title,
    software_peer_review = "Software Peer Review" %in% yaml$tags,
    tech_note = "tech notes" %in% yaml$tags && !"Software Peer Review" %in% yaml$tags,
    other = !"tech notes" %in% yaml$tags && !"Software Peer Review" %in% yaml$tags,
    twitterImg = yaml$twitterImg %||% "",
    twitterAlt = yaml$twitterAlt %||% "",
    description = yaml$description %||% "",
    newsletter = "newsletter" %in% yaml$tags,
    slug = yaml$slug,
    dir = fs::path_dir(path),
    language = language(path)
    )

  post_url <- if (meta[["language"]] == "en") {
    sprintf(
      "/blog/%s/%s/%s/%s",
      lubridate::year(meta$date),
      stringr::str_pad(lubridate::month(meta$date), 2, "0", side = "left"),
      stringr::str_pad(lubridate::day(meta$date), 2, "0", side = "left"),
      meta$slug
    )
  } else {
    sprintf(
      "/%s/blog/%s/%s/%s/%s",
      meta[["language"]],
      lubridate::year(meta$date),
      stringr::str_pad(lubridate::month(meta$date), 2, "0", side = "left"),
      stringr::str_pad(lubridate::day(meta$date), 2, "0", side = "left"),
      meta$slug
    )
  }

  meta$url <- post_url
  meta
}
paths <- fs::dir_ls("..", recurse = TRUE, glob = "*.md")
paths <- paths[!paths %in% c("../_index.md", "../2021-02-03-targets/raw_data_source.md",
  "../2021-02-03-targets/README.md")]
posts <- purrr::map_df(paths, parse_one_post)
posts <- dplyr::filter(posts, date >= as.Date(last_newsletter), !newsletter)
posts <- split(posts, posts[["dir"]])
format_post <- function(dir) {
  main_language <- if (any(dir[["language"]] == "en")) {
    "en"
  } else {
    dir[["language"]][[1]]
  }
  
  post <- dir[which(dir[["language"]] == main_language),]
  string <- sprintf("* [%s](%s) by %s", post$title, url, post$author)
  if (post$description != "") {
    string <- paste0(string, ". ", sub("\\?$", "", sub("\\!$", "", sub("\\.$", "", post$description), ".")), ".")
  } else {
    string <- paste0(string, ".")  
  }
  
  if (post$twitterImg != "") {
    img_file <- fs::path_file(post$twitterImg)
    download.file(sprintf("https://ropensci.org/%s", post$twitterImg), img_file)
    img_file %>% magick::image_read() %>% magick::image_scale("400x") %>% magick::image_write(img_file)
    string <- paste0(
      string,
      sprintf('{{< figure src="%s" alt="%s" width="400" >}}\n\n', img_file, post$twitterAlt)
    )
  }
  
  other_langs <- dir[which(dir[["language"]] != main_language),]
  other_langs <- split(other_langs, sort(as.numeric(rownames(other_langs))))
  if (length(other_langs) > 0) {
    other_langs_text <- purrr::map_chr(
      other_langs,
      ~ sprintf("<a href='%s' lang='%s'>%s (%s)</a>", .x[["url"]], .x[["language"]], .x[["title"]], .x[["language"]])
      ) %>% 
      toString
    other_langs_text <- sprintf("Other languages: %s.", other_langs_text)
    string <- sprintf("%s %s", string, other_langs_text)
  }
  
  string
}
```

```{r, results='asis'}
software_review <- posts[purrr::map_lgl(posts, ~any(.x[["software_peer_review"]]))]
if (length(software_review) > 0) {
  cat("### Software Review\n\n")
  cat(
    paste0(
      purrr::map_chr(software_review, format_post),
      collapse = "\n\n"
    )
  )
  cat("\n\n")
}

others <- posts[purrr::map_lgl(posts, ~any(.x[["other"]]))]
if (length(others) > 0) {
  if (length(others) != length(posts)) cat("### Other topics\n\n")
  cat(
    paste0(
      purrr::map_chr(others, format_post),
      collapse = "\n\n"
    )
  )
  cat("\n\n")
}


tech_notes <- posts[purrr::map_lgl(posts, ~any(.x[["tech_note"]]))]
if (length(tech_notes) > 0) {
  cat("\n\n")
  cat("### Tech Notes\n\n")
  cat(
    paste0(
      purrr::map_chr(tech_notes, format_post),
      collapse = "\n\n"
    )
  )
  cat("\n\n")
}
```

<!-- ## Use cases

```{r usecases, eval=FALSE}
# rerun get_use_cases.R at the same time
usecases <- jsonlite::read_json("../../../data/usecases/usecases.json")
get_one_case <- function(usecase) {
  tibble::tibble(
    title = usecase$title,
    reporter = usecase$reporter,
    url = usecase$url,
    image = usecase$image,
    date = anytime::anydate(usecase$date)
  )
}
usecases <- purrr::map_df(usecases, get_one_case)
usecases <- dplyr::filter(usecases, date >= as.Date(last_newsletter))
usecases <- split(usecases, seq(nrow(usecases)))
```

`snakecase::to_sentence_case(english(length(usecases)))` use case` if (length(usecases) > 1) "s"` of our packages and resources ha` if (length(usecases) > 1) "ve" else "s"` been reported since we sent the last newsletter.

```{r usecases2, results='asis', eval=FALSE}
format_case <- function(usecase) {
  string <- sprintf("* [%s](%s). Reported by %s.", sub("\\.$", "", usecase$title), usecase$url, usecase$reporter)
}
cat(
  paste0(
    purrr::map_chr(usecases, format_case),
    collapse = "\n\n"
  )
)
```

Explore [other use cases](/usecases) and [report your own](https://discuss.ropensci.org/c/usecases/10)! -->

## Call for (co)maintainers

### Call for maintainers

If you're interested in maintaining any of the R packages below, you might enjoy reading our blog post [What Does It Mean to Maintain a Package?](/blog/2023/02/07/what-does-it-mean-to-maintain-a-package/) (or listening to its discussion on the [R Weekly highlights podcast](https://rweekly.fireside.fm/111) hosted by Eric Nantz and Mike Thomas)!

- **[rvertnet](https://cran.r-project.org/web/packages/rvertnet/index.html)**, Retrieve, map and summarize data from the VertNet.org archives (<http://vertnet.org/>). Functions allow searching by many parameters, including taxonomic names, places, and dates. In addition, there is an interface for conducting spatially delimited searches, and another for requesting large datasets via email. [Issue for volunteering](https://github.com/ropensci-archive/rvertnet/issues/71).

- **[natserv](https://cran.r-project.org/web/packages/natserv/index.html)**. Interface to NatureServe (<https://www.natureserve.org/>). Includes methods to get data, image metadata, search taxonomic names, and make maps. [Issue for volunteering](https://github.com/ropensci-archive/natserv/issues/29).

- **[geojsonlint](https://cran.r-project.org/web/packages/geojsonlint/index.html)**, Tools for linting GeoJSON. Includes tools for interacting with the online tool <http://geojsonlint.com>, the Javascript library geojsonhint (<https://www.npmjs.com/package/geojsonhint>), and validating against a GeoJSON schema via the Javascript library (<https://www.npmjs.com/package/is-my-json-valid>). Some tools work locally while others require an internet connection. [Issue for volunteering](https://github.com/ropensci-archive/geojsonlint/issues/22).

- **[citesdb](https://docs.ropensci.org/citesdb/)**, a high-performance database of shipment-level CITES trade data. Provides convenient access to over 40 years and 20 million records of
endangered wildlife trade data from the Convention on International Trade
in Endangered Species of Wild Fauna and Flora, stored on a local on-disk,
out-of memory 'DuckDB' database for bulk analysis. [Issue for volunteering](https://github.com/ropensci/citesdb/issues/21).


### Call for comaintainers

Refer to our [recent blog post](/blog/2022/10/17/maintain-or-co-maintain-an-ropensci-package/#packages-looking-for-co-maintainers) to identify other packages where help is especially wished for!

## Package development corner

Some useful tips for R package developers. :eyes:

### Caching the results of functions of your R package

The R-hub blog has a post on [Caching the results of functions of your R package](https://blog.r-hub.io/2021/07/30/cache/) by MaÃ«lle Salmon and Christophe Dervieux.
It presents different approaches.
A recent PR by Lukas Wallrich to rcrossref maintained by Najko Jahn illustrates the approach of [saving results to an environment](https://github.com/ropensci/rcrossref/pull/237) (and reminded us about the topic of caching!).

### Deprecated data in a package with `delayedAssign()`

If you need to change something like a function or function arguments in your package, you'll find many resources in, say, the [rOpenSci dev guide](https://devguide.ropensci.org/evolution.html), or the [lifecycle package docs](https://lifecycle.r-lib.org/).
Now what if you want to deprecate a **dataset**?
Read the [solution proposed by Matthijs Berends on Stack Overflow](https://stackoverflow.com/questions/33304651/data-deprecation-in-r-package/75192818#75192818), linking to [a 3-step method in Bioconductor guidance](https://contributions.bioconductor.org/deprecation.html#deprecate-dataset).
Key is the usage of `delayedAssign()` to save a promise that will serve both a warning, and the data.
We might mention this approach in a future version of our [dev guide](https://github.com/ropensci/dev_guide/issues/649).

You might remember `delayedAssign()` from a [strategy to provide data as tibble only when tibble is installed](/blog/2022/01/21/ropensci-news-digest-january-2022/#how-to-provide-tibbles-only-to-those-who-like-them).
Useful function!

### Important change for CRAN packages with compiled code

Andy Teucher, R Package Developer Educator at Posit PBC, wrote a post about ["New CRAN requirements for packages with C and C++"](https://www.tidyverse.org/blog/2023/03/cran-checks-compiled-code/).
Good to know for packages on CRAN, or to be submitted to CRAN, with compiled code!

### Let dependabot monitor updates to your GitHub Actions

A tip by Jacob Wujciak-Jens.
Dependabot alerts are really easy to set up to monitor for updates to actions, like this [example](https://github.com/ropensci-review-tools/pkgcheck/blob/main/.github/dependabot.yml). 
See the [Full GitHub docs](https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuring-dependabot-version-updates#enabling-dependabot-version-updates).

## Last words

Thanks for reading! If you want to get involved with rOpenSci, check out our [Contributing Guide](https://contributing.ropensci.org) that can help direct you to the right place, whether you want to make code contributions, non-code contributions, or contribute in other ways like sharing use cases.

If you haven't subscribed to our newsletter yet, you can [do so via a form](/news/). Until it's time for our next newsletter, you can keep in touch with us via our [website](/) and [Mastodon account](https://hachyderm.io/@rOpenSci).
